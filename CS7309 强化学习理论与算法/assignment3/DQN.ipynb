{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNnet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        '''Args:\n",
    "            state_dim  int: state data's last dimension\n",
    "            hidden_dim  List[int]: hidden dimension of every hidden layer\n",
    "            action_dim  int:output action data's last dimension  \n",
    "        '''\n",
    "        super(DQNnet, self).__init__()\n",
    "        self.num_layers = len(hidden_dim) + 1\n",
    "        self.layers = nn.ModuleList(nn.Linear(in_channels, out_channels) for in_channels, out_channels in zip([state_dim] + hidden_dim, hidden_dim + [action_dim]))\n",
    "        self.__init_parameters__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx < self.num_layers - 1:\n",
    "                x = F.leaky_relu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __init_parameters__(self):\n",
    "        # initialize the parameters of the modules\n",
    "        for p in self.layers.parameters():\n",
    "            if p.dim() > 1:\n",
    "                init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' experience buffer for DQN learning '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        transitions = random.sample(self.buffer, sample_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "class DQN_agent:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update_frequency, device, mode='naive'):\n",
    "        self.target_DQN_net = DQNnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.DQN_net = DQNnet(state_dim, hidden_dim, action_dim).to(device)  \n",
    "\n",
    "        self.target_DQN_net.to(device)\n",
    "        self.DQN_net.to(device)\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.mode = mode\n",
    "        self.optimizer = torch.optim.Adam(self.DQN_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # epsilon-greedy rate\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "        self.count = 0  # counter for step num\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "            if self.mode == 'naive':\n",
    "                action = torch.argmax(self.DQN_net(state)).item()\n",
    "            else:\n",
    "                action = torch.argmax(self.target_DQN_net(state)).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        # get split record data\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        max_next_q_values = self.target_DQN_net(next_states).max(dim=1)[0].view(-1, 1)\n",
    "        # here become a single value tensor\n",
    "        q_values = self.DQN_net(states).gather(1, actions)  # Q(s,a)\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss = F.mse_loss(q_values, q_targets)\n",
    "        dqn_loss.backward() \n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update_frequency == 0:\n",
    "            self.target_DQN_net.load_state_dict(self.DQN_net.state_dict())\n",
    "        self.count += 1\n",
    "\n",
    "    def save(self, output_dir):\n",
    "        output_path = os.path.join(output_dir, f\"{self.mode}_DQN_net.pth\")\n",
    "        torch.save(self.DQN_net, output_path)\n",
    "    \n",
    "    def load(self, model_dir):\n",
    "        model_path = os.path.join(model_dir, f\"{self.mode}_DQN_net.pth\")\n",
    "        self.DQN_net = torch.load(model_path)\n",
    "        self.target_DQN_net = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: DQN_agent, buffer: ReplayBuffer, env: gym.Env, epochs: int, sample_size: int, maxstep: int = 1000, minimal_size: int = 0, save_dir = \"model\"):\n",
    "    return_list = []\n",
    "    num_bar = 20\n",
    "    update_frequency = 10\n",
    "    best_return = -sys.maxsize - 1\n",
    "    for bar_idx in range(num_bar):\n",
    "        with tqdm(total=int(epochs / num_bar), desc=f'Iteration {bar_idx+1}') as pbar:\n",
    "            for episode in range(int(epochs / num_bar)):\n",
    "                episode_return = 0\n",
    "                state, _ = env.reset()\n",
    "                done = False\n",
    "                # do actions to get an episode train\n",
    "                for step in range(maxstep):\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    if reward == 0:\n",
    "                        virtual_reward = 1000\n",
    "                    elif next_state[1] > 0:\n",
    "                        virtual_reward = -1 +  4000 * next_state[1] * next_state[1]\n",
    "                    else:\n",
    "                        virtual_reward = -1\n",
    "                    buffer.add(state, action, virtual_reward, next_state, done)\n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                    # for stability, only when buffer size > minimal_size, agent will update parameters\n",
    "                    if buffer.size() > minimal_size:\n",
    "                        states, actions, rewards, next_states, dones = buffer.sample(sample_size)\n",
    "                        transition_dict = {\n",
    "                            'states': states,\n",
    "                            'actions': actions,\n",
    "                            'rewards': rewards,\n",
    "                            'next_states': next_states,\n",
    "                            'dones': dones\n",
    "                        }\n",
    "                        agent.update(transition_dict)\n",
    "                    if done:\n",
    "                        break\n",
    "                if best_return < episode_return:\n",
    "                    agent.save(save_dir)\n",
    "                    best_return = episode_return\n",
    "                pbar.set_postfix({'episode': f'{epochs / num_bar * bar_idx + episode + 1}', 'return': f'{episode_return}'})\n",
    "                pbar.update(1)    \n",
    "\n",
    "                return_list.append(episode_return)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent: DQN_agent, env: gym.Env, test_num: int):\n",
    "    total_reward = 0\n",
    "    total_count = 0\n",
    "    for epoch in range(test_num):\n",
    "        epoch_reward = 0\n",
    "        state,_ = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "        for step in range(200):\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, _ ,_ = env.step(action)\n",
    "            state = next_state\n",
    "            epoch_reward += reward\n",
    "            if done:\n",
    "                total_count += 1\n",
    "                break\n",
    "        total_reward += epoch_reward\n",
    "\n",
    "    env.close()\n",
    "    return total_count, total_reward / test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "mode_list = ['naive', 'double']\n",
    "lr = 1e-2\n",
    "epochs = 100\n",
    "state_dim = env.observation_space.shape[0]\n",
    "hidden_dim = [16, 4]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "target_update_frequency = 10\n",
    "buffer_size = 200\n",
    "minimal_size = 50\n",
    "maxstep = 1000\n",
    "sample_size = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed = 42\n",
    "\n",
    "test_num = 10\n",
    "\n",
    "# seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_list = dict()\n",
    "eval_reward_list = dict()\n",
    "\n",
    "for mode_name in mode_list:\n",
    "    print(\"Now we starts to train {} mode\".format(mode_name))\n",
    "    # train the agent\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    agent = DQN_agent(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n",
    "                target_update_frequency, device, mode=mode_name)\n",
    "\n",
    "    returns = train(agent, replay_buffer, env, epochs, sample_size, maxstep=maxstep, minimal_size=minimal_size)\n",
    "    returns_list[mode_name] = returns\n",
    "\n",
    "for mode_name in mode_list:\n",
    "    # draw figure of rewards curve\n",
    "    episodes_list = range(len(returns_list[mode_name]))\n",
    "    plt.plot(episodes_list, returns_list[mode_name], label=mode_name)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.title(f'{mode_name} DQN algorithm performance')\n",
    "    fig_name = f'{mode_name}_compare'\n",
    "    plt.savefig(fig_name)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode_name in mode_list:\n",
    "    agent = DQN_agent(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n",
    "                target_update_frequency, device, mode=mode_name)\n",
    "    agent.load(\"model\")\n",
    "    # evaluate the agent\n",
    "    eval_env = gym.make(env_name)\n",
    "    total_count, avg_reward = evaluate(agent, eval_env, test_num)\n",
    "    eval_reward_list[mode_name] = avg_reward\n",
    "print(total_count, eval_reward_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
